\section{Discussion/Conclusions}
\label{sec:disc}
In this manuscript, a 3D CNN for automatic segmentation of the prostate and the zonal anatomy on T2 MRIs, collected from two MRI-vendors is presented. This is a continuation of our previous work, where a multi-atlas based segmentation method was evaluated also for different MRI vendors. In both projects we used the same crisscross design, whereby a method trained with imaging data from one vendor was tested in segmenting images acquired on another. Similarly, the 3D CNN and atlas-based segmentation improved when images from both vendors were used for training. 

The objective of the paper was to train a network that will perform robustly in large variety of images and make it available to the community (the software is freely available at \url{http://git@github.com:olmozavala/Prostate_and_PZ_DL_Segmentation_Code}). To the best of our knowledge, our developments utilized the largest collection of annotated prostate imaging datasets. It should be noted that even though the CNN was trained and tested on T2-w MRIs with sizes $168^3$, the final model is not restricted to that input size and could be used with different resolutions as long as the field of view of the input volume is similar to the training data and the size of the image can be divided by 8 without a reminder ($104^3$,$160^3$, $168^3$, $184^3$, $240^3$, etc). As a test, we ran images at $200^3$ and the network segmented the data successfully (data not shown). While comparing 3D CNN to the atlas-based approach was not a goal of the work, it should be noted that 3D CNN outperformed both prostate and PZ segmentation.  

This paper brings to the forefront the need of assessment of data heterogeneity, when comparing the performance of segmentation methods. While intuitively self-evident, this factor is often ignored in the quest for higher DSC scores. A robust performance of a segmentation with DCS in the $0.85$ range maybe more desirable than a DSC $> 0.9$ of a model trained in homogeneous dataset. For example, as evident from Table \ref{tab:dataset}, our Siemens data is more homogeneous. The Siemens' trained model performed with high accuracy on Siemens image data (Table \ref{tab:res_prost}, DCS $\sim0.89$), but ostensibly failed on GE (DSC $< 0.3$). Meanwhile, the GE trained model reached a more modest DSC for the GE data (DSC $\sim0.86$), but performed quite robustly on Siemens data. 
For these reasons, comparing the performance of our network with other CNN approaches is not straightforward. Mooij et al. \cite{mooij_automatic_2018} report DSC of $0.85$ and $0.65$ for TZ and PZ, respectively. The network was trained with fifty-three 3D T2-w datasets. Our results are superior, however, this most likely is a consequence of the larger training dataset. Meyer et al. \cite{anneke} used a subset of forty images from ProstateX to train a network, resulting in average DCS for segmenting the prostate of $92.1\%$. Possible explanation for these excellent results is again the homogeneity of this data, as exemplified in Table \ref{tab:dataset}. Tian et al. \cite{to2018deep} tested their model with two distinct datasets and obtained DSCs of $95.11$ and $89.01$. This large difference of more than $6\%$ in the performance of the network is attributable to the variation in heterogeneity of each dataset. 

This study has some limitations. Even though the manual segmentation was performed by experienced operators, there is inherent error in the "ground truth" contours. The limited inter-reader study, carried in our previous work \cite{deukwoo_classification_2018}, yielded DSC of $0.88$ and $0.66$ for the prostate and PZ, respectively. These results are similar to previously reported \cite{4_klein2008automatic}. In this light, how can we explain the higher DSC from CNN? There is a variety of factors that affect the process of prostate segmentation. While our network was trained on a very large dataset, there is no guarantee that this dataset encompasses all possible variations related to anatomical variability between subjects, as well as variability in imaging data. The established network could be used as a basis and then fine tuned to a particular classifier on top of this CNN for a new dataset \cite{tajbakhsh2016convolutional}.
